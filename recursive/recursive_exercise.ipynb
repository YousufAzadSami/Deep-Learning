{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Network Exercise\n",
    "\n",
    "In this exercise, you should train a recursive neural network which can estimate the _free energy_ of an _RNA secondary structure_. In biology, RNA sequences fold to so-called secondary structures and it is assumed that secondary structures are preferred which have little free energy. Free energy is minimized if base pairs are joined in stable pairs.\n",
    "\n",
    "For this task, though, you do not need to know anything about the actual biological specifics. You can just train a recursive neural net which infers the correct energy (a simple scalar) from a given tree.\n",
    "\n",
    "### Report\n",
    "\n",
    "For the report, please describe the architecture that you used to solve the task and generate the following plot. After training the network, generate 100 further trees and record for each tree the size using the `recursive_oracle.tree_size` function and the error `abs(y - y_predicted)`. Plot error against tree size in a scatter plot.\n",
    "\n",
    "<strong>Note:</strong> Please use the `exercise_sheet_template.tex` to generate your report. Your report is due on *Friday, March 15th, 10am* as single-page PDF to [aschulz@techfak.uni-bielefeld.de](mailto:aschulz@techfak.uni-bielefeld.de). Please start your e-mail subject with the words *[Deep Learning]*.\n",
    "\n",
    "### Advice\n",
    "\n",
    "Do not try to map directly to the energy because a one-dimensional encoding may carry too little information. Rather, apply a recursive neural network to a low-dimensional encoding space first and then another neural network which predicts from the encoding the free energy.\n",
    "\n",
    "Further, this predictive task is not super easy, so do not try to achieve perfect error values. If you manage to stay consistently below an error of 1 this is already a good result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tree pair(g, pair(g, pair(c, hairpin(a, hairpin(a, hairpin(g, hairpin_end(g)))), g), u), u) has energy value 2.06355\n"
     ]
    }
   ],
   "source": [
    "# For this exercise, we already provide data generation function (an 'oracle')\n",
    "# which we can use\n",
    "from recursive_oracle import generate_rna_tree\n",
    "\n",
    "# let's have a look at an example tree and its energy value.\n",
    "# Executing this cell multiple times will yield different trees.\n",
    "x, y = generate_rna_tree()\n",
    "print('the tree %s has energy value %g' % (str(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RecursiveNet(torch.nn.Module):\n",
    "    def __init__(self, dim, arity_alphabet):\n",
    "        \n",
    "        super(RecursiveNet, self).__init__()\n",
    "        self.dim = int(dim)\n",
    "        self.arity_alphabet = arity_alphabet\n",
    "        self.embed = torch.nn.Embedding(len(arity_alphabet), dim)\n",
    "        self.noise = torch.nn.Embedding(len(arity_alphabet), dim)\n",
    "        self.constants = torch.nn.ParameterDict()\n",
    "        self.layers = torch.nn.ModuleDict()\n",
    "        self.l1 = torch.nn.Linear(2 * dim, dim)\n",
    "\n",
    "        for symbol, arity in self.arity_alphabet.items():\n",
    "            if(arity == 0):\n",
    "                self.constants[symbol] = torch.nn.Parameter(torch.randn(self.dim))\n",
    "            else:\n",
    "                self.layers[symbol] = torch.nn.Linear(arity * self.dim, self.dim)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, T):\n",
    "        \n",
    "        arity = self.arity_alphabet[T.label]\n",
    "        if(len(T.children) != arity):\n",
    "            raise ValueError('Expected %s children for a node with label %s but got %d children.' % (\n",
    "                arity, T.label, len(T.children)))\n",
    "        if(arity == 0):\n",
    "            return self.sigmoid(self.constants[T.label])\n",
    "        child_encodings = []\n",
    "        for child in T.children:\n",
    "            child_encodings.append(self.forward(child))\n",
    "        child_encodings = torch.cat(child_encodings)\n",
    "        encoding = self.layers[T.label](child_encodings)\n",
    "        #encoding = self.embed(Variable(torch.LongTensor([self.arity_alphabet[T.label]])))\n",
    "        #encoding = self.sigmoid(encoding)\n",
    "        encoding = self.sigmoid(encoding)\n",
    "        return encoding\n",
    "    \n",
    "k = 1\n",
    "rna_arity_alphabet = {'dangle_end' : 1, 'dangle' : 2, 'split' : 2,'branch' : 2,'pair' : 3,'hairpin' : 2,\n",
    "                  'hairpin_end' : 1,'c' : 0, 'g' : 0, 'a' : 0, 'u' : 0}\n",
    "model = RecursiveNet(k, rna_arity_alphabet)\n",
    "\n",
    "loss_function = torch.nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "#torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dangle': 2, 'dangle_end': 1, 'split': 2, 'pair': 3, 'branch': 2, 'hairpin': 2, 'hairpin_end': 1, 'c': 0, 'g': 0, 'a': 0, 'u': 0}\n"
     ]
    }
   ],
   "source": [
    "# The oracle also provides us with the arity alphabet for the RNA trees\n",
    "from recursive_oracle import rna_arity_alphabet\n",
    "\n",
    "print(rna_arity_alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 20 batches: 3.0897516340017317\n",
      "loss after 40 batches: 3.20043983399868\n",
      "loss after 60 batches: 3.1519632995128632\n",
      "loss after 80 batches: 2.974375388622284\n",
      "loss after 100 batches: 3.3466975355148314\n",
      "loss after 120 batches: 2.699699021577835\n",
      "loss after 140 batches: 2.7163452661037444\n"
     ]
    }
   ],
   "source": [
    "loss_threshold = 1\n",
    "learning_curve = []\n",
    "\n",
    "minibatch_size = 50\n",
    "\n",
    "while(not learning_curve or learning_curve[-1] > loss_threshold):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss_batch = 0\n",
    "    for i in range(minibatch_size):\n",
    "        (x, y) = generate_rna_tree()\n",
    "        #print(x)\n",
    "        y_predicted = model(x)\n",
    "        loss_object = loss_function(y, y_predicted)\n",
    "        loss_batch += loss_object.item()\n",
    "        loss_object.backward()\n",
    "\n",
    "    learning_curve.append(loss_batch / minibatch_size)\n",
    "    if(len(learning_curve) % 20 == 0):\n",
    "        print('loss after {} batches: {}'.format(len(learning_curve), learning_curve[-1]))\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list(range(len(learning_curve))), learning_curve)\n",
    "plt.xlabel('gradient step')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights for symbol 'dangle' = tensor([0.7311, 0.3511]) ; bias = tensor(0.0624)\n",
      "weights for symbol 'dangle_end' = tensor([-0.8864]) ; bias = tensor(-0.9605)\n",
      "weights for symbol 'split' = tensor([0.0690, 0.2204]) ; bias = tensor(0.0242)\n",
      "weights for symbol 'pair' = tensor([-0.0332,  0.3772, -0.1551]) ; bias = tensor(-0.6845)\n",
      "weights for symbol 'branch' = tensor([ 0.9058, -0.1659]) ; bias = tensor(0.4337)\n",
      "weights for symbol 'hairpin' = tensor([0.2573, 1.1539]) ; bias = tensor(0.7086)\n",
      "weights for symbol 'hairpin_end' = tensor([0.1667]) ; bias = tensor(1.0580)\n",
      "encoding for symbol 'c' = Parameter containing:\n",
      "tensor([2.2922], requires_grad=True)\n",
      "encoding for symbol 'g' = Parameter containing:\n",
      "tensor([-0.2355], requires_grad=True)\n",
      "encoding for symbol 'a' = Parameter containing:\n",
      "tensor([1.5517], requires_grad=True)\n",
      "encoding for symbol 'u' = Parameter containing:\n",
      "tensor([0.0582], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for symbol, arity in rna_arity_alphabet.items():\n",
    "    if(arity > 0):\n",
    "        w = model.layers[symbol].weight.data[0]\n",
    "        b = model.layers[symbol].bias.data[0]\n",
    "        print('weights for symbol \\'%s\\' = %s ; bias = %s' % (symbol, str(w), str(b)))\n",
    "    else:\n",
    "        print('encoding for symbol \\'%s\\' = %s' % (symbol, str(model.constants[symbol])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
